#!/usr/bin/perl
exec ("perldoc", $0);
exit 1;

=head1 NAME

whintro - Introduction to "warehouse" cloud computing facilities

=head1 SYNOPSIS

The warehouse provides facilities for parallel computation (see
L</BATCH JOBS>) and data storage (see L</STORAGE>).  These are
accessed using command-line utilities (see L</PROGRAMS>) or the Perl
library (see Warehouse(3pm)).

=head1 BATCH JOBS

You can run programs on other nodes in the cloud by storing them in a
subversion repository and submitting a job to the cloud controller.

Batch jobs offer several advantages over running programs on your own
local machine:

=over

=item *

Increase B<concurrency> by running tasks asynchronously, using many
CPUs and network interfaces at once (especially beneficial for
CPU-bound and I/O-bound tasks respectively).

=item *

Track inputs, outputs, and settings so you can verify that the
B<sequence of programs> you used to arrive at an output is really what
you think it was.

=item *

Ensure that your programs and workflows are B<repeatable> with
different versions of your code, OS updates, etc.

=item *

B<Interrupt and resume> long-running jobs consisting of many short
steps.

=item *

Maintain B<timing> statistics automatically, so they're there when you
want them.

=back

=head2 STRUCTURE OF BATCH JOBS

A batch job consists of a number of B<steps> which can be executed
asynchronously.

A single batch job program, or "mr-function", executes each B<step> of
a given job.  The logic of a typical mr-function looks like this:

=over

=item *

If this is the first step: examine the input, divide it into a number
of asynchronous tasks, instruct the jobmanager (via stderr) to queue
these tasks, output nothing, and indicate successful completion.

=item *

Otherwise, fetch our portion of the input from the cloud storage
system, do some computation, store some output in the cloud, output a
fragment of the output manifest, and indicate successful completion.

=back

When all job steps have completed, the output fragments are assembled
into a single output manifest.

If a job step fails, it is automatically re-attempted.  If the same
step fails 3 times, any steps still running are allowed to complete,
and the job is abandoned.

=head2 WRITING BATCH JOBS

Usually, it makes sense to test your mr-function locally on small data
sets.  When you are satisfied that it works, commit it to the
subversion repository and try running it on the cloud.

Check out a working copy of the subversion repository.

Write your mr-function in {repo}/mapreduce/mr-foo

Test your function:

 {repo}/warehouse/server/whjobmanager revision={repo} mrfunction=foo ....

The arguments to whjobmanager(1p) are the same as the arguments to "wh
job new" (see wh(1p)) with these exceptions:

=over

=item *

B<revision=/path/to/repo> specifies a checked-out (and possibly
modified) copy of the repository instead of a subversion revision
number.

=item *

B<nodes> and B<photons> are not required.

=back

You will see the progress of the job in your terminal.  Press
Control-C to create a checkpoint and stop the job.

=head2 RUNNING BATCH JOBS

See wh(1p) for details about submitting batch jobs.

=head1 STORAGE

The storage system is suitable for short-term and long-term storage
(see L</VALUABLE DATA>).  It is optimized for medium and large data
sets (megabytes to terabytes).  Small data sets work too: performance
won't be as good as a local RAID-1 filesystem, but convenience and
durability still apply.

The storage system arranges data into B<manifests> and B<data blocks>.

A B<manifest> is analogous to a directory in a traditional filesystem.
It contains subdirectories and filenames, and indicates where to find
the data blocks which comprise the files.  It is stored in plain text.

A B<data block> contains between 1 byte and 64 MiB of data.  Its MD5
checksum is used as an identifier/locator.

The storage system automatically distributes data blocks among the
available disks.  It also stores multiple copies of each block, so a
single disk or node failure does not cause any data to become
unreachable.  It prevents users from overwriting one another's (and
their own) data.

=head1 VALUABLE DATA

Valuable data blocks must be marked explicitly by registering a
B<name> with the cloud controller; otherwise they will eventually be
deleted to make room for new data.

To indicate that a data block is valuable, you need to attach a
B<name> to either the locator of the block itself, or the locator of a
manifest which contains a reference to the block.

Use the wh(1p) program to name your valuable data.  For example:

 wh manifest name name=/sample/name/ newkey=ecd39d68e3db311b841dc4af4f4a59ed

=head1 PROGRAMS

B<wh> submits, controls, and inspects batch processing jobs and the
cloud's list of named data objects.

B<whget> retrieves data from the cloud's storage system and stores it
on the local filesystem (or stdout).  B<whless> is convenient for
inspecting data in your terminal.

B<whput> copies data from the local filesystem (or stdin) to the
cloud's storage system.

B<whence> figures out how a given data object was (or could be)
computed, based on the history of batch jobs that have run on the
cloud.

B<whjobmanager> runs a mr-function on the local machine.  This is
useful for writing and debugging mr-functions, and for running short
jobs when the cloud is to busy to get a node allocation.

=cut
